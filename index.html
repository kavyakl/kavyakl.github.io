<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Lakshmi Kavya Kalyanam</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 900px;
      margin: 40px auto;
      line-height: 1.6;
      color: #222;
    }
    h1 { margin-bottom: 0; }
    h2 { margin-top: 40px; }
    a { color: #0056b3; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>

<h1>Lakshmi Kavya Kalyanam</h1>
<p><strong>GPU/ML Engineer | Compiler-Aware Optimization | Embedded AI Systems</strong></p>

<p>
PhD candidate specializing in hardware-aware neural network optimization,
sparse training, and ONNX graph transformations for edge and GPU inference.
My work bridges model compression and deployment through compiler-inspired
techniques and low-level performance engineering.
</p>

<h2>Research</h2>
<ul>
  <li>MLIR-inspired compression pipelines achieving high sparsity with minimal accuracy drop</li>
  <li>Activation-aware MLP pruning for microcontroller deployment</li>
  <li>ONNX graph rewrites enabling structure-preserving sparse acceleration</li>
</ul>

<h2>Selected Projects</h2>
<ul>
  <li><strong>ONNX Graph Rewrite Engine</strong> – Compiler-style IR transformations for sparse model deployment</li>
  <li><strong>Distributed Edge Inference</strong> – 3-node PYNQ-Z1 real-time object detection framework</li>
  <li><strong>Sparse CNN Training (RigL)</strong> – Dynamic sparsity with 4× FLOP reduction</li>
</ul>

<h2>Publications</h2>
<p>Selected peer-reviewed publications available upon request.</p>

<h2>Contact</h2>
<p>
Email: kavyakalyanamk@gmail.com <br>
GitHub: <a href="https://github.com/kavyakl">github.com/kavyakl</a><br>
LinkedIn: <a href="https://linkedin.com/in/lakshmikavya-kalyanam-a88633131">LinkedIn Profile</a>
</p>

</body>
</html>